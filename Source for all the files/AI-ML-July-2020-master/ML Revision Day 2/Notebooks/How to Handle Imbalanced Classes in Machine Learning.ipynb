{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Handle Imbalanced Classes in Machine Learning?\n",
    "\n",
    "Imbalanced classes put “accuracy” out of business. This is a surprisingly common problem in machine learning (specifically in classification), occurring in datasets with a disproportionate ratio of observations in each class.\n",
    "\n",
    "Standard accuracy no longer reliably measures performance, which makes model training much trickier.\n",
    "\n",
    "Imbalanced classes appear in many domains, including:\n",
    "\n",
    "Fraud detection\n",
    "Spam filtering\n",
    "Disease screening\n",
    "SaaS subscription churn\n",
    "Advertising click-throughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Intuition: Disease Screening Example:__\n",
    "\n",
    "Let’s say your client is a leading research hospital, and they’ve asked you to train a model for detecting a disease based on biological inputs collected from patients.\n",
    "\n",
    "But here’s the catch… the disease is relatively rare; it occurs in only 8% of patients who are screened.\n",
    "\n",
    "Unfortunately, that accuracy is misleading.\n",
    "\n",
    "For patients who do not have the disease, you’d have 100% accuracy.\n",
    "\n",
    "For patients who do have the disease, you’d have 0% accuracy.\n",
    "\n",
    "Your overall accuracy would be high simply because most patients do not have the disease (not because your model is any good).\n",
    "\n",
    "This is clearly a problem because many machine learning algorithms are designed to maximize overall accuracy. The rest of this guide will illustrate different tactics for handling imbalanced classes.\n",
    "\n",
    "__Note:__  Not every technique below will work for every problem. However, 9 times out of 10, at least one of these techniques should do the trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Balance Scale Dataset:__\n",
    "\n",
    "For this guide, we’ll use a synthetic dataset called Balance Scale Data, which you can download from the UCI Machine Learning Repository \"http://archive.ics.uci.edu/ml/datasets/balance+scale\".\n",
    "\n",
    "This dataset was originally generated to model psychological experiment results, but it’s useful for us because it’s a manageable size and has imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  balance  var1  var2  var3  var4\n",
       "0       B     1     1     1     1\n",
       "1       R     1     1     1     2\n",
       "2       R     1     1     1     3\n",
       "3       R     1     1     1     4\n",
       "4       R     1     1     1     5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# Read dataset\n",
    "df = pd.read_csv('balance-scale.data', \n",
    "                 names=['balance', 'var1', 'var2', 'var3', 'var4'])\n",
    " \n",
    "# Display example observations\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains information about whether a scale is balanced or not, based on weights and distances of the two arms.\n",
    "\n",
    "It has 1 target variable, which we've labeled balance .\n",
    "\n",
    "It has 4 input features, which we've labeled var1  through var4 .\n",
    "\n",
    "The target variable has 3 classes.\n",
    "\n",
    "R for right-heavy, i.e. when var3 * var4 > var1 * var2\n",
    "\n",
    "L for left-heavy, i.e. when var3 * var4 < var1 * var2\n",
    "\n",
    "B for balanced, i.e. when var3 * var4 = var1 * var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    288\n",
       "L    288\n",
       "B     49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count of each class\n",
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for our purpose, we're going to turn this into a binary classification problem.\n",
    "\n",
    "We're going to label each observation as 1 (positive class) if the scale is balanced or 0 (negative class) if the scale is not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balance</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   balance  var1  var2  var3  var4\n",
       "0        1     1     1     1     1\n",
       "1        0     1     1     1     2\n",
       "2        0     1     1     1     3\n",
       "3        0     1     1     1     4\n",
       "4        0     1     1     1     5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform into binary classification\n",
    "df['balance'] = [1 if b=='B' else 0 for b in df.balance]\n",
    " \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    576\n",
       "1     49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['balance'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, only about 8% of the observations were balanced. Therefore, if we were to always predict 0, we'd achieve an accuracy of 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Danger of Imbalanced Classes:__\n",
    "\n",
    "Now that we have a dataset, we can really show the dangers of imbalanced classes.\n",
    "\n",
    "First, let's import the Logistic Regression algorithm and the accuracy metric from Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train\n",
    "# Separate input features (X) and target variable (y)\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_0 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_0 = clf_0.predict(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As mentioned above, many machine learning algorithms are designed to maximize overall accuracy by default.\n",
    "\n",
    "We can confirm this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9216\n"
     ]
    }
   ],
   "source": [
    "# How's the accuracy?\n",
    "print( accuracy_score(pred_y_0, y) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So our model has 92% overall accuracy, but is it because it's predicting only 1 class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Should we be excited?\n",
    "print( np.unique( pred_y_0 ) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you can see, this model is only predicting 0, which means it's completely ignoring the minority class in favor of the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the technique for handling imbalanced classes: \n",
    "\n",
    "# 1. Up-sampling the minority class:\n",
    "\n",
    "Up-sampling is the process of randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "\n",
    "There are several heuristics for doing so, but the most common way is to simply resample with replacement.\n",
    "\n",
    "First, we'll import the resampling module from Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for resampling\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a new DataFrame with an up-sampled minority class. Here are the steps:\n",
    "\n",
    ">1. First, we'll separate observations from each class into different DataFrames.\n",
    "\n",
    ">2. Next, we'll resample the minority class with replacement, setting the number of samples to match that of the majority class.\n",
    "\n",
    ">3. Finally, we'll combine the up-sampled minority class DataFrame with the original majority class DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    576\n",
       "0    576\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df[df.balance==0]\n",
    "df_minority = df[df.balance==1]\n",
    " \n",
    "# Upsample minority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=576,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    " \n",
    "# Display new class counts\n",
    "df_upsampled.balance.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As you can see, the new DataFrame has more observations than the original, and the ratio of the two classes is now 1:1.\n",
    "\n",
    "Let's train another model using Logistic Regression, this time on the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.5147569444444444\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = df_upsampled.balance\n",
    "X = df_upsampled.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_1 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_1 = clf_1.predict(X)\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_1 ) )\n",
    "# [0 1]\n",
    " \n",
    "# How's our accuracy?\n",
    "print( accuracy_score(y, pred_y_1) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Great, now the model is no longer predicting just one class. While the accuracy also took a nosedive, it's now more meaningful as a performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Down-sample Majority Class:\n",
    "\n",
    "Down-sampling involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm.\n",
    "\n",
    "The most common heuristic for doing so is resampling without replacement.\n",
    "\n",
    "The process is similar to that of up-sampling. Here are the steps:\n",
    "\n",
    ">1. First, we'll separate observations from each class into different DataFrames.\n",
    "\n",
    ">2. Next, we'll resample the majority class without replacement, setting the number of samples to match that of the minority class.\n",
    "\n",
    ">3. Finally, we'll combine the down-sampled majority class DataFrame with the original minority class DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    49\n",
       "0    49\n",
       "Name: balance, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate majority and minority classes\n",
    "df_majority = df[df.balance==0]\n",
    "df_minority = df[df.balance==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=49,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "df_downsampled.balance.value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This time, the new DataFrame has fewer observations than the original, and the ratio of the two classes is now 1:1.\n",
    "\n",
    "Again, let's train a model using Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.5612244897959183\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = df_downsampled.balance\n",
    "X = df_downsampled.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_2 = LogisticRegression().fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_2 = clf_2.predict(X)\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_2 ) )\n",
    "# [0 1]\n",
    " \n",
    "# How's our accuracy?\n",
    "print( accuracy_score(y, pred_y_2) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The model isn't predicting just one class, and the accuracy seems higher.\n",
    "\n",
    "We'd still want to validate the model on an unseen test dataset, but the results are more encouraging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Penalize Algorithms (Cost-Sensitive Training):\n",
    "\n",
    "The next tactic is to use penalized learning algorithms that increase the cost of classification mistakes on the minority class.\n",
    "\n",
    "A popular algorithm for this technique is Penalized-SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "During training, we can use the argument class_weight='balanced'  to penalize mistakes on the minority class by an amount proportional to how under-represented it is.\n",
    "\n",
    "We also want to include the argument probability=True  if we want to enable probability estimates for SVM algorithms.\n",
    "\n",
    "Let's train a model using Penalized-SVM on the original imbalanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.688\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_3 = SVC(kernel='linear', \n",
    "            class_weight='balanced', # penalize\n",
    "            probability=True)\n",
    " \n",
    "clf_3.fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_3 = clf_3.predict(X)\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_3 ) )\n",
    " \n",
    "# How's our accuracy?\n",
    "print( accuracy_score(y, pred_y_3) )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Again, our purpose here is only to illustrate this technique. To really determine which of these tactics works best for this problem, you'd want to evaluate the models on a hold-out test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Use Tree-Based Algorithms\n",
    "\n",
    "The final tactic we'll consider is using tree-based algorithms. Decision trees often perform well on imbalanced datasets because their hierarchical structure allows them to learn signals from both classes.\n",
    "\n",
    "In modern applied machine learning, tree ensembles (Random Forests, Gradient Boosted Trees, etc.) almost always outperform singular decision trees, so we'll jump right into those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now, let's train a model using a Random Forest on the original imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Separate input features (X) and target variable (y)\n",
    "y = df.balance\n",
    "X = df.drop('balance', axis=1)\n",
    " \n",
    "# Train model\n",
    "clf_4 = RandomForestClassifier()\n",
    "clf_4.fit(X, y)\n",
    " \n",
    "# Predict on training set\n",
    "pred_y_4 = clf_4.predict(X)\n",
    " \n",
    "# Is our model still predicting just one class?\n",
    "print( np.unique( pred_y_4 ) )\n",
    " \n",
    "# How's our accuracy?\n",
    "print( accuracy_score(y, pred_y_4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! 97% accuracy? Is this magic? A sleight of hand? Cheating? Too good to be true?\n",
    "\n",
    "Well, tree ensembles have become very popular because they perform extremely well on many real-world problems. They are certainly recommended them wholeheartedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__However:__\n",
    "\n",
    "While these results are encouraging, the model could be overfit, so you should still evaluate your model on an unseen test set before making the final decision.\n",
    "\n",
    "Note: your numbers may differ slightly due to the randomness in the algorithm. You can set a random seed for reproducible results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Recommended Homework:__\n",
    "\n",
    "# Create Synthetic Samples (Data Augmentation)\n",
    "\n",
    "Creating synthetic samples is a close cousin of up-sampling, and some people might categorize them together. For example, the SMOTE algorithm is a method of resampling from the minority class while slightly perturbing feature values, thereby creating \"new\" samples.\n",
    "\n",
    "You can find an implementation of SMOTE in the imblearn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Happy Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
